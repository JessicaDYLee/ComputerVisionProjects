{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohansameer1983/ComputerVisionProjects/blob/main/Trash_Classification_CV_Neptune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYtHaghPZIA3",
        "outputId": "b0bca37d-b925-4dc8-fd36-01d610cea75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.15.1.tar.gz (315 kB)\n",
            "\u001b[K     |████████████████████████████████| 315 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting neptune-tensorflow-keras\n",
            "  Downloading neptune-tensorflow-keras-0.9.9.tar.gz (20 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Collecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.3.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 66.4 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.21.16-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Collecting swagger-spec-validator>=2.7.4\n",
            "  Downloading swagger_spec_validator-2.7.4-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.25.0,>=1.24.16\n",
            "  Downloading botocore-1.24.16-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 54.3 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.16->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 74.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (3.13)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.4.0)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.1.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.7)\n",
            "Building wheels for collected packages: neptune-client, future, neptune-tensorflow-keras\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.15.1-py2.py3-none-any.whl size=564805 sha256=3b95c4b757a4393914307fe505992cf9f337a283e47b56f2856e25414df61b92\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/8b/b6/b935fc912f5a61ecaad6024ad68ac9295ba7f5d9e3c8cf5728\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0d3afe4e1c20b5c1ab42d86a6f8860bec0bd4800a81796cdf0aabe57558c3d7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for neptune-tensorflow-keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-tensorflow-keras: filename=neptune_tensorflow_keras-0.9.9-py3-none-any.whl size=5283 sha256=293414150ef3e193472f16a2c72a8c13c72d940b44445f872816bc938e407cc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/f7/43/e0ddf424b3535bc207729eb8a5da81478963603bd2fbf0accf\n",
            "Successfully built neptune-client future neptune-tensorflow-keras\n",
            "Installing collected packages: arrow, webcolors, urllib3, uri-template, rfc3987, rfc3339-validator, jsonpointer, jmespath, isoduration, fqdn, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, tf-estimator-nightly, neptune-client, neptune-tensorflow-keras\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 PyJWT-2.3.0 arrow-1.2.2 boto3-1.21.16 botocore-1.24.16 bravado-11.0.3 bravado-core-5.17.0 fqdn-1.5.1 future-0.18.2 gitdb-4.0.9 isoduration-20.11.0 jmespath-0.10.0 jsonpointer-2.2 jsonref-0.2 monotonic-1.6 neptune-client-0.15.1 neptune-tensorflow-keras-0.9.9 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.5.2 simplejson-3.17.6 smmap-5.0.0 swagger-spec-validator-2.7.4 tf-estimator-nightly-2.8.0.dev2021122109 uri-template-1.1.0 urllib3-1.25.11 webcolors-1.11.1 websocket-client-1.3.1\n"
          ]
        }
      ],
      "source": [
        "# Uncomment only for first run\n",
        "!pip install neptune-client neptune-tensorflow-keras tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hZj4c0xTeMwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db04824-169f-49fb-9469-cbc3ef6c2cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.8.0\n",
            "Hub version: 0.12.0\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "import neptune.new as neptune\n",
        "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow      # To show images using cv2 module\n",
        "\n",
        "import pathlib\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jI50JlncLBuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358aedc2-172e-481a-bba9-adb292cc8800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/mohansameer/Trash-Classification-Deep-Learning/e/TRAS-13\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ],
      "source": [
        "run = neptune.init(\n",
        "    project=\"mohansameer/Trash-Classification-Deep-Learning\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5ZjI5MDY4Yi0zMWVlLTQ3MmMtYjE4NS1iNDRjNGM0OTg4ZDkifQ==\",\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QruPz2sHIoZC"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4LoocvGWInOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0787ba86-1ecd-4510-ba6e-8608caecc419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: resnet_v2_101 : https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\n",
            "Input size (224, 224)\n"
          ]
        }
      ],
      "source": [
        "  # Select base model  for transfer learning\n",
        "  model_name = \"resnet_v2_101\" # @param [ 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152',  'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "  model_handle_map = {\n",
        "    \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "    \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "    \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "    \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "    \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\",\n",
        "    \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "    \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "    \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "    \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "    \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "    \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "    \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "    \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "    \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "    \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "    \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "  }\n",
        "\n",
        "  PIXELS = 224#@param {type:\"integer\"}\n",
        "  BATCH_SIZE = 16#@param {type:\"integer\"}\n",
        "\n",
        "  model_handle = model_handle_map.get(model_name)\n",
        "\n",
        "  print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "  IMAGE_SIZE = (PIXELS, PIXELS)\n",
        "  print(f\"Input size {IMAGE_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gTV9nmxaLKci"
      },
      "outputs": [],
      "source": [
        "#Log to NeptuneAI\n",
        "run[\"model/transferlearning/model_name\"] = model_name\n",
        "run[\"model/transferlearning/model_url\"] = model_handle\n",
        "run[\"model/transferlearning/model_batch_size\"] = BATCH_SIZE\n",
        "run[\"model/parameters/image_size\"] = IMAGE_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHvJJjnCRYF2"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7591PrXxL5qB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8010f60-5a7b-40e0-96e3-693a99bc1b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive so dataset can be accessed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q3gYLKfEd0Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72d5418-06ff-44e6-8354-c2e3cd2bc4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 288 images belonging to 28 classes.\n",
            "Found 1500 images belonging to 28 classes.\n"
          ]
        }
      ],
      "source": [
        "def load_data(save_dir=\"./\"):\n",
        "  data_dir = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/data/trash') \n",
        "\n",
        "  datagen_kwargs = dict(rescale=1./255, validation_split=0.2)\n",
        "  dataflow_kwargs = dict(target_size=(PIXELS, PIXELS), \n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        interpolation=\"nearest\")\n",
        "\n",
        "  valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      **datagen_kwargs)\n",
        "\n",
        "  valid_generator = valid_datagen.flow_from_directory(\n",
        "      data_dir, subset=\"validation\", \n",
        "      shuffle=False, **dataflow_kwargs)\n",
        "  \n",
        "  train_datagen = ImageDataGenerator(\n",
        "            horizontal_flip=True,\n",
        "            **datagen_kwargs)\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          data_dir,\n",
        "          class_mode='categorical',\n",
        "          **dataflow_kwargs)\n",
        "  return valid_generator, train_generator\n",
        "\n",
        "VAL_generator, TRAIN_generator = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-eWMH8Mi8ibt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30394194-c243-4af9-9b49-d9883d329bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('Aluminium foil', 0), ('Battery', 1), ('Blister pack', 2), ('Bottle', 3), ('Bottle cap', 4), ('Broken glass', 5), ('Can', 6), ('Carton', 7), ('Cigarette', 8), ('Cup', 9), ('Food waste', 10), ('Glass jar', 11), ('Lid', 12), ('Other plastic', 13), ('Paper', 14), ('Paper bag', 15), ('Plastic bag & wrapper', 16), ('Plastic container', 17), ('Plastic glooves', 18), ('Plastic utensils', 19), ('Pop tab', 20), ('Rope & strings', 21), ('Scrap metal', 22), ('Shoe', 23), ('Squeezable tube', 24), ('Straw', 25), ('Styrofoam piece', 26), ('Unlabeled litter', 27)])\n"
          ]
        }
      ],
      "source": [
        "class_names = TRAIN_generator.class_indices.items()\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2USajN2MWjn"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "## Build and Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GZfFboF85rIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87e9320-90e9-439d-f239-38456757b4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model with https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 2048)              42626560  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               262272    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 28)                3612      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42,892,444\n",
            "Trainable params: 265,884\n",
            "Non-trainable params: 42,626,560\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_model(trainable=False, params={}):\n",
        "   print(\"Building model with\", model_handle)\n",
        "   model = keras.Sequential([\n",
        "        keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "        hub.KerasLayer(model_handle, trainable=trainable),\n",
        "        keras.layers.Dense(params['dense_units_1'],activation=params['dense_layer_activation_1']),\n",
        "        keras.layers.Dropout(params['dropout']),\n",
        "        keras.layers.Dense(TRAIN_generator.num_classes,\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(params['regularizer_rate']))])\n",
        "   model.build((None,)+IMAGE_SIZE+(3,))\n",
        "   model.summary()\n",
        "   return model\n",
        "\n",
        "params = {\n",
        "    \"dense_units_1\": 128,\n",
        "    \"dense_layer_activation_1\": \"relu\",\n",
        "    \"regularizer_rate\":0.005,\n",
        "    \"dropout\": 0.1,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"n_epochs\": 15,\n",
        "}\n",
        "run[\"model/parameters\"] = params\n",
        "model = build_model(params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z3EyvQbSzu5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d7e581-f5f5-4fdf-9ed7-16dd58b7fd5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "def compile_model(model):\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(lr=params['learning_rate'], momentum=0.8), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = compile_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz8kT3f8zykl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9904f905-22a4-41e8-b28e-56a53d9a9d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "93/93 [==============================] - 1127s 12s/step - loss: 3.1681 - accuracy: 0.2210 - val_loss: 2.7087 - val_accuracy: 0.3090\n",
            "Epoch 2/15\n",
            "93/93 [==============================] - 222s 2s/step - loss: 2.7753 - accuracy: 0.3147 - val_loss: 2.3981 - val_accuracy: 0.4028\n",
            "Epoch 3/15\n",
            "93/93 [==============================] - ETA: 0s - loss: 2.5622 - accuracy: 0.3571"
          ]
        }
      ],
      "source": [
        "def train_model(model, num_epochs):\n",
        "\n",
        "    steps_per_epoch = TRAIN_generator.samples // TRAIN_generator.batch_size\n",
        "    validation_steps = VAL_generator.samples // VAL_generator.batch_size\n",
        "\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    history = model.fit(\n",
        "      TRAIN_generator,\n",
        "      epochs=num_epochs, steps_per_epoch=steps_per_epoch,\n",
        "      validation_data = VAL_generator,\n",
        "      validation_steps = validation_steps,\n",
        "      callbacks=[NeptuneCallback(run=run)]).history\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "model, history = train_model(model, num_epochs=params['n_epochs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9JAgWLwHQbQ"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(history[\"loss\"])\n",
        "plt.plot(history[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(history[\"accuracy\"])\n",
        "plt.plot(history[\"val_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLyG7AqZH2Q2"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32IylFqtH1MQ"
      },
      "outputs": [],
      "source": [
        "saved_model_path = \"/content/drive/MyDrive/Colab Notebooks/Models/saved_garbageclassification_model\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Trash_Classification_CV_Neptune.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}